from abc import ABC, abstractmethod

from torch import Tensor
from torch.nn import functional as F

__all__ = ["DivergenceMetricFactory", "DivergenceMetricMixin", "KLDivergence", "SEDivergence", "SupportDivergence"]

class DivergenceMetricMixin(ABC):
    @abstractmethod
    def __call__(self, log_probs: Tensor, target_probs: Tensor) -> Tensor:
        """
        Compute the divergence between the output of the policy network and the behavior policy
        """
        raise NotImplementedError

class KLDivergence(DivergenceMetricMixin):
    """
    Kullback-Leibler Divergence
    """
    def __call__(self, log_probs: Tensor, target_probs: Tensor) -> Tensor:
        # KL Loss: L(y_pred, y_true) = y_true * (log(y_true) - log(y_pred))
        return F.kl_div(log_probs, target_probs, reduction='none', log_target=True).sum(dim=1)

    
# class DivergenceThreshold(DivergenceMetricMixin):
#     """
#     penalty = 1 if Divergence > threshold
#     """
#     def __init__(self, metric: DivergenceMetricMixin, threshold: float) -> None:
#         super().__init__()
#         self.metric = metric
#         self.threshold = threshold

#     def __call__(self, log_probs: Tensor, target_probs: Tensor) -> Tensor:
#         divergence = self.metric(log_probs, target_probs)
#         return (divergence > self.threshold).float()

class SEDivergence(DivergenceMetricMixin):
    """
    Squared Error Divergence
    """
    def __call__(self, log_probs: Tensor, target_probs: Tensor) -> Tensor:
        return (target_probs - log_probs.exp()).pow(2).sum(dim=1)
    
class SupportDivergence(DivergenceMetricMixin):
    """
    p(s,a) = 0 if = if behavior_policy(a | s) >= eps
    Does not use the probabilities generated by the actor network
    Only punishes Q values
    """
    # Some large penalty value
    INFINITY = 1e2

    def __init__(self, epsilon) -> None:
        super().__init__()
        self.epsilon = epsilon

    def __call__(self, log_probs: Tensor, target_probs: Tensor) -> Tensor:
        """
        target_probs: behavior_policy(a | s)
        probs: policy(a | s)
        """
        # TODO: Fix, only look at actual action not target probs
        return (target_probs < self.epsilon).float() * self.INFINITY
    
class DivergenceMetricFactory:
    def __init__(self, name: str) -> None:
        self.name = name

    def create(self) -> DivergenceMetricMixin:
        if self.name == "se":
            return SEDivergence()
        elif self.name == "kl":
            return KLDivergence()
        else:
            raise ValueError(f"Unknown divergence metric: {self.name}")